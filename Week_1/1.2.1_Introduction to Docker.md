# A. Introduction to Docker

## üö∞ What is a Data Pipeline?
- A data pipeline is like an assembly line for data: it ingests, processes, transforms, and delivers data to where it‚Äôs needed ‚Äî clean, reliable, and ready to use.
- It could look like the below where the source is a CSV file. There is a python script ingesting and transforming the data and then it is deposited into a table in Postgres.
- Bear in mind that this could similarly also be a chain of smaller processes of this chained together.

## What is Docker?
- Docker is a set of service products that deliver software in isolated packages called containers.
- Docker uses these **containers** to isolate parts of a data pipeline from the rest of the host machine. 
- Each container includes everything it needs to run e.g. below table (code, dependencies, environment), so it won‚Äôt interfere with other services 
‚Äî for example, you can run a separate instance of PostgreSQL in a container without clashing with one installed on your system.


| Tool/Library                    | Role in the Pipeline                                                                 |
|--------------------------------|---------------------------------------------------------------------------------------|
| **Ubuntu 20.04**               | The base OS of the container. It's a lightweight Linux environment everything runs on. |
| **Python 3.9**                 | The main language used to write pipeline logic ‚Äî data ingestion, transformation, etc. |
| **Pandas**                     | A powerful data manipulation library. Used to clean, transform, and analyze data.     |
| **PostgreSQL library** (e.g. `psycopg2`) | Allows Python to connect to a Postgres database, run queries, and load/save data.      |


## üì¶ What is a Docker Image?
- A Docker image is like a blueprint or snapshot of your app or data pipeline environment ‚Äî it includes the code, libraries, dependencies, and configs all bundled together.
- You build an image once (e.g., a container that runs dbt + PostgreSQL).
- Then you can run it anywhere ‚Äî on your laptop, in the cloud, or in Kubernetes on GCP/AWS ‚Äî and it works exactly the same.
- "Build once, run anywhere" is Docker's magic ‚ú®

## üîÅ How It Works in Practice
- Develop locally in a Docker container.
- Push the image to a container registry (like Docker Hub, AWS ECR, or GCP Artifact Registry).
- Deploy it in Kubernetes, AWS ECS, or anywhere else that supports containers.

## üö¢ Why Docker Containers?
| Reason                      | Explanation                                                                                      |
|-----------------------------|--------------------------------------------------------------------------------------------------|
| **Reproducibility**         | Run the same pipeline anywhere ‚Äî avoids "it works on my machine" problems.                      |
| **Local Experimentation**   | Safely test code and dependencies in isolated environments.                                     |
| **CI/CD & Testing**         | Enables integration tests in pipelines via GitHub Actions, Jenkins, etc.                        |
| **Cloud Execution**         | Deploy containers to AWS Batch, Kubernetes, ECS, etc.                                           |
| **Isolation**               | Run multiple versions of tools (e.g., Python, Postgres) without conflict.                       |
| **Packaging**               | Bundle all code, libraries, and configs into a single portable image.                           |
| **Serverless Compatibility**| Run as a container in serverless environments like AWS Lambda or Google Cloud Functions.        |
| **Tool Integration**        | Works well with tools like Airflow, dbt, and Spark.                                              |
| **Industry Standard**       | Containers are a must-know skill for modern Data Engineers.                                     |


## Running Docker

## ‚úÖ Testing Docker Installation

After installing Docker and navigating to your working repository, test the installation by running:

```bash
docker run hello-world
```

You should see a message like:

```bash
Hello from Docker!
This message shows that your installation appears to be working correctly.
```

**What happened:**

- Docker pulled the image from Docker Hub.
- A container was created and executed.
- You saw the expected "Hello from Docker!" message.

---

## üêß Running an Ubuntu Container

```bash
docker run -it ubuntu bash
```

- Runs the container interactively with a terminal.
- `ubuntu` is the image.
- `bash` starts a Bash shell inside the container.

---

## üêö Running a Bash Image

```bash
docker run -it bash
```

- Pulls and runs the `bash` image.
- You get dropped into a container running Bash.
- Basic Linux directories (`/bin`, `/etc`, etc.) are present.

To exit the container:

```bash
exit
```

---

## üêç Running a Python 3.9 Container

```bash
docker run -it python:3.9
```

- Pulls the `python:3.9` image.
- Opens an interactive Python shell inside the container.

Try it:

```python
print('hello world')
import pandas  # ‚ùå Fails! pandas not installed.
```

---

## üì¶ Installing Packages inside the Container

To gain access to a shell for installing packages:

```bash
docker run -it --entrypoint=bash python:3.9
```

- Overrides the default entrypoint (Python shell) with `bash`.
- Allows package installation and system exploration.

Install pandas:

```bash
pip install pandas
```

Then:

```bash
python
>>> import pandas
>>> pandas.__version__
'2.2.3'
```

‚úÖ **Success!** `pandas` is now available **in this container**.

---

## üß† Important Note on Container Persistence

Containers do **not** retain changes unless you:

- Commit them to a new image.
- Use volumes or Dockerfiles to rebuild.

So if you start a new container:

```bash
docker run -it --entrypoint=bash python:3.9
```

And try:

```python
import pandas
```

‚ùå It will **fail again** ‚Äî because pandas isn't saved across containers.

---

## üí° Key Docker Concepts Covered

| Concept         | Description                                              |
|----------------|----------------------------------------------------------|
| `docker run`   | Runs a container from an image.                          |
| `-it`          | Interactive terminal mode.                               |
| `bash`         | Shell inside the container.                              |
| `--entrypoint` | Overrides the image‚Äôs default startup command.           |
| Image          | A snapshot of an environment (e.g., Python, Ubuntu).     |
| Container      | A running instance of an image.                          |

## üîç What is a Dockerfile?

- A Dockerfile is a text document that contains a series of instructions on how to build a Docker image. 
- Essentially, it automates the process of setting up an environment for your application or tool, so you don‚Äôt have to manually configure everything each time.
### üõ†Ô∏è How Does a Dockerfile Work?
- When you build a Docker image from a Dockerfile, Docker reads the instructions one by one and executes them to create a custom image that contains everything needed to run your application. 
- Once the image is built, it can be used to create containers that run your app or services.
### Simple Example of a Dockerfile
```
# Use an official Python runtime as a parent or base image
FROM python:3.9

# Install any needed dependencies
RUN pip install pandas

# We can also specify a startpoint rather than python as bash
ENTRYPOINT ["bash"]
```

We can then build it via the terminal like this:
```bash
docker build -t test:pandas .
```
This will show it installing dependencies

# C. Writing the Docker File

A Dockerfile defines how to build a Docker image for your pipeline. Here‚Äôs the provided example:

```dockerfile
FROM python:3.9
RUN pip install pandas
WORKDIR /app
COPY pipeline.py pipeline.py
ENTRYPOINT ["python", "pipeline.py"]
```

### Breakdown

| Instruction                         | Purpose                                      |
| ------------------------------------ | -------------------------------------------- |
| `FROM python:3.9`                    | Uses Python 3.9 as the base image.           |
| `RUN pip install pandas`             | Installs pandas for data processing.        |
| `WORKDIR /app`                       | Sets /app as the working directory.         |
| `COPY pipeline.py pipeline.py`       | Copies pipeline.py into /app.               |
| `ENTRYPOINT ["python", "pipeline.py"]` | Runs pipeline.py when the container starts. |

## Building the Image

Build with:

```bash
docker build -t my-pipeline:latest .
```

This creates an image named `my-pipeline:latest`. Verify with:

```bash
docker images
```

---

# D. Passing Args into the Container

The `pipeline.py` script expects a day argument:

```python
import sys
import pandas as pd

print(f"Arguments are: {sys.argv}")
day = sys.argv[1]
print(f"Job Finished Successfully for day = {day}")
```

### Running with Arguments

Pass arguments via `docker run`:

```bash
docker run my-pipeline:latest 2025-04-13
```

Output:

```
Arguments are: ['pipeline.py', '2025-04-13']
Job Finished Successfully for day = 2025-04-13
```

### Handling Missing Arguments

Without arguments, `pipeline.py` fails (`IndexError`). Add error handling:

```python
if len(sys.argv) < 2:
    print("Error: Please provide a day argument.")
    sys.exit(1)
```

Run without args:

```bash
docker run my-pipeline:latest
```

Output:

```
Arguments are: ['pipeline.py']
Error: Please provide a day argument.
```

---

## Automation

For CI/CD (e.g., Kubernetes), pass args dynamically via job configs or environment variables.

---

## üí° Key Points

- **Dockerfile**: Sets up the pipeline environment and entrypoint.
- **Arguments**: Pass via `docker run <image> <arg>`.
- **Robustness**: Handle missing args in `pipeline.py`.
