# 1. GCP
## What is GCP
Cloud Computing services offered by Google.
Range of hosted services for compute, storage and application development that run on Google Hardware.

We will focus on Big Data and Storage and Databases but there is a wide range of services offered by Google as below:

![GCP Services](/images/GCPServiceOverview.png)

# 2. Introduction to Docker

## üí° Key Concepts:

- Docker lets you run apps/services in isolated environments

- Think of it like a "lightweight VM" ‚Äî same app, same setup every time, anywhere

- A Docker container runs from a Docker image

- **Images** are like blueprints; **containers** are the running copies

## üì¶ Why this matters for data engineers:

We‚Äôll use Docker to:
  - Run services locally (e.g., Airflow, Postgres)
  - Share  work with others easily
  - Avoid ‚Äúworks on my machine‚Äù issues

## What is Docker?
- Docker is a set of service products that deliver software in isolated packages called containers.

- Docker uses these **containers** to isolate parts of a data pipeline from the rest of the host machine. 

- Each container includes everything it needs to run e.g. below table (code, dependencies, environment), so it won‚Äôt interfere with other services 

- for example, you can run a separate instance of PostgreSQL in a container without clashing with one installed on your system.


| Tool/Library                    | Role in the Pipeline                                                                 |
|--------------------------------|---------------------------------------------------------------------------------------|
| **Ubuntu 20.04**               | The base OS of the container. It's a lightweight Linux environment everything runs on. |
| **Python 3.9**                 | The main language used to write pipeline logic ‚Äî data ingestion, transformation, etc. |
| **Pandas**                     | A powerful data manipulation library. Used to clean, transform, and analyze data.     |
| **PostgreSQL library** (e.g. `psycopg2`) | Allows Python to connect to a Postgres database, run queries, and load/save data.      |


## üì¶ What is a Docker Image?
- A Docker image is like a blueprint or snapshot of your app or data pipeline environment ‚Äî it includes the code, libraries, dependencies, and configs all bundled together.
- You build an image once (e.g., a container that runs dbt + PostgreSQL).
- Then you can run it anywhere ‚Äî on your laptop, in the cloud, or in Kubernetes on GCP/AWS ‚Äî and it works exactly the same.
- "Build once, run anywhere" is Docker's magic ‚ú®

## üö¢ Why Docker Containers?

1. **Reproducibility**  
   Run the same pipeline anywhere ‚Äî avoids "it works on my machine" problems.

2. **Local Experimentation**  
   Safely test code and dependencies in isolated environments.

3. **CI/CD & Testing**  
   Enables integration tests in pipelines via GitHub Actions, Jenkins, and more.

4. **Cloud Execution**  
   Deploy containers to cloud platforms like AWS Batch, Kubernetes, or ECS.

5. **Isolation**  
   Run multiple versions of tools (e.g., Python, Postgres) without conflict.

6. **Packaging**  
   Bundle all code, libraries, and configs into a single portable image.

7. **Serverless Compatibility**  
   Run as a container in serverless environments like AWS Lambda or Google Cloud Functions.

8. **Tool Integration**  
   Works seamlessly with orchestration tools like Airflow, dbt, and Spark.

9. **Industry Standard**  
   Containers are a must-know skill for modern Data Engineers.

