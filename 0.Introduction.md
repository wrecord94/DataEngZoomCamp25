# INTRODUCTION
## üß† What is Data Engineering?
Data Engineering is all about building the systems that collect, move, clean, store, and serve data ‚Äî so that analysts, data scientists, and apps can use it.
Data engineers are a bit like the plumbers of the data world ‚Äî they make sure the data flows smoothly from source to destination, and it's clean, reliable, and fast.

## How the Course will be structured

![Course_Plan_Image](images/course_plan.png)

## üß± The Core Aspects of Data Engineering

### 1. Data Ingestion - Getting data in from different sources.
- Could be from APIs, databases, files, message queues, or real-time streams.
- Example: Pulling surf data from an external API every hour.
- Tools: Python, Airbyte, Kafka, REST APIs, FTP, etc.

### 2. Data Storage - Saving the raw or processed data somewhere.
- Can be databases (PostgreSQL, BigQuery), data lakes (S3, GCS), or warehouses (Snowflake, Redshift).
- You choose depending on structure, size, cost, and access speed.
- Concepts:
  - Data Lakes (store everything)
  - Data Warehouses (structured & fast querying)

### 3. Data Transformation - Turning raw data into clean, usable formats.
- Cleaning, formatting, combining, deduplicating, aggregating, etc.
- This is where tools like SQL and dbt shine.
- ETL vs ELT:
  - ETL: Extract ‚Üí Transform ‚Üí Load
  - ELT: Extract ‚Üí Load ‚Üí Transform (modern approach)

### 4. Orchestration & Scheduling - Automating all the steps above on a regular basis.
- Chaining your jobs together and running them at the right times or in the right order.
- Example: ‚ÄúIngest ‚û° Transform ‚û° Load ‚û° Notify‚Äù
- Tools: Airflow, Prefect, Dagster

### 5. Monitoring & Reliability - Making sure pipelines don‚Äôt silently fail.
- Add logging, alerts, retry logic.
- Think like an engineer: if a pipeline fails at 2am, how do you know?
- Tools: Prometheus, Grafana, Airflow‚Äôs built-in monitoring

### 6. Infrastructure / DevOps (Optional but Powerful) - Knowing how your systems run, scale, and deploy.
- Not always required at junior level, but powerful to understand:
  - Docker for containerization
  - GitHub Actions / CI/CD for testing & deploying
  - Cloud (AWS, GCP) for hosting and scaling

## üß∞ Common Tech Stack
| Layer          | Tool Examples                                           |
|----------------|---------------------------------------------------------|
| Ingestion      | Python, Kafka, Airbyte                                  |
| Storage        | S3, PostgreSQL, BigQuery, Snowflake                     |
| Transformation | SQL, dbt, Pandas                                        |
| Orchestration  | Airflow, Prefect                                        |
| Monitoring     | Airflow UI, Prometheus, custom logs                     |
| Deployment     | Docker, Terraform, GitHub Actions                       |
| Cloud          | AWS (S3, Lambda, RDS), GCP (BigQuery, Cloud Functions)  |


## üë∂ Junior Data Engineer Role ‚Äì What You‚Äôll Typically Do
- Write basic data pipelines (Python scripts, SQL jobs)
- Maintain and monitor scheduled jobs
- Work with analysts or scientists to get them the data they need
- Debug data issues and fix broken pipelines
- Slowly grow into working on architecture, tooling, and infrastructure

### üéì Tip as You Start the Course
Don‚Äôt worry about understanding everything at once. Focus on the ‚Äúwhy‚Äù behind each tool or technique. The Zoomcamp will show you:
	‚Ä¢ How each part fits in a real pipeline
	‚Ä¢ How to work with cloud and containers
How to build production-grade systems step-by-step