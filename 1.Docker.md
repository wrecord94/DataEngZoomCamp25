# Introduction to Docker

## üö∞ What is a Data Pipeline?
- A data pipeline is like an assembly line for data: it ingests, processes, transforms, and delivers data to where it‚Äôs needed ‚Äî clean, reliable, and ready to use.
- It could look like the below where the source is a CSV file. There is a python script ingesting and transforming the data and then it is deposited into a table in Postgres.
- Bear in mind that this could similarly also be a chain of smaller processes of this chained together.

## What is Docker?
- Docker is a set of service products that deliver software in isolated packages called containers.
- Docker uses these **containers** to isolate parts of a data pipeline from the rest of the host machine. 
- Each container includes everything it needs to run e.g. below table (code, dependencies, environment), so it won‚Äôt interfere with other services 
‚Äî for example, you can run a separate instance of PostgreSQL in a container without clashing with one installed on your system.


| Tool/Library                    | Role in the Pipeline                                                                 |
|--------------------------------|---------------------------------------------------------------------------------------|
| **Ubuntu 20.04**               | The base OS of the container. It's a lightweight Linux environment everything runs on. |
| **Python 3.9**                 | The main language used to write pipeline logic ‚Äî data ingestion, transformation, etc. |
| **Pandas**                     | A powerful data manipulation library. Used to clean, transform, and analyze data.     |
| **PostgreSQL library** (e.g. `psycopg2`) | Allows Python to connect to a Postgres database, run queries, and load/save data.      |


## üì¶ What is a Docker Image?
- A Docker image is like a blueprint or snapshot of your app or data pipeline environment ‚Äî it includes the code, libraries, dependencies, and configs all bundled together.
- You build an image once (e.g., a container that runs dbt + PostgreSQL).
- Then you can run it anywhere ‚Äî on your laptop, in the cloud, or in Kubernetes on GCP/AWS ‚Äî and it works exactly the same.
- "Build once, run anywhere" is Docker's magic ‚ú®

## üîÅ How It Works in Practice
- Develop locally in a Docker container.
- Push the image to a container registry (like Docker Hub, AWS ECR, or GCP Artifact Registry).
- Deploy it in Kubernetes, AWS ECS, or anywhere else that supports containers.

## üö¢ Why Docker Containers?
| Reason                      | Explanation                                                                                      |
|-----------------------------|--------------------------------------------------------------------------------------------------|
| **Reproducibility**         | Run the same pipeline anywhere ‚Äî avoids "it works on my machine" problems.                      |
| **Local Experimentation**   | Safely test code and dependencies in isolated environments.                                     |
| **CI/CD & Testing**         | Enables integration tests in pipelines via GitHub Actions, Jenkins, etc.                        |
| **Cloud Execution**         | Deploy containers to AWS Batch, Kubernetes, ECS, etc.                                           |
| **Isolation**               | Run multiple versions of tools (e.g., Python, Postgres) without conflict.                       |
| **Packaging**               | Bundle all code, libraries, and configs into a single portable image.                           |
| **Serverless Compatibility**| Run as a container in serverless environments like AWS Lambda or Google Cloud Functions.        |
| **Tool Integration**        | Works well with tools like Airflow, dbt, and Spark.                                              |
| **Industry Standard**       | Containers are a must-know skill for modern Data Engineers.                                     |


## Running Docker
## Writing the Docker File
## Passing Args into the Container