# LESSON 1: What Is Data Engineering?
## üß† Goal:
Understand what a data engineer actually does, and how data systems fit together.

## 1Ô∏è‚É£ What Problem Are Data Engineers Solving
Imagine a company has apps or services creating tons of data (user clicks, orders, transactions. The company will also have Analysts or data scientists who want insights from that data.
The problem is the the raw data is messy, huge, and scattered across systems. It needs to be:
- Collected
- Cleaned
- Organized
- Delivered

Enter the data engineer. You‚Äôre the one who builds and maintains the pipelines that move and transform data.

# The Data Engineering Pipeline

üîÑ This flow is called a data pipeline ‚Äî and your job is to make sure it works reliably, efficiently, and automatically.

![DataFlowDiagram](/images/DataFlowDiagram.png)

## Note: the difference between "Storage" vs "Data Warehouse"
üì¶ Storage (aka Data Lake)
This is the raw data it can be in many formats so this is Files ‚Äî CSV, JSON, Parquet, Avro.
It is usually stored using tech like GCS (Google Cloud Storage), AWS S3, Azure Blob as these are Cheap, scalable, flexible. They also importantly don‚Äôt require structure ‚Äî can dump anything here.

**Example:** You download a surf forecast JSON from an API every day and save it to a Data Lake ‚Äî it‚Äôs just sitting there waiting to be processed.

## üè¢ Data Warehouse

Is different this data has been transformed and processed into a structured format which is optimized for queries. Tables with schemas.

The tech used will be BigQuery, Snowflake, Redshift, Azure Synapse. These allow for fast SQL queries across big data. They are designed for analytics (BI tools, dashboards, reports)

üß† Use when: the data is cleaned, modeled, and ready to be used for decisions

## 3Ô∏è‚É£ Real-World Examples

**Netflix:** Pipelines ingest viewing data to recommend shows
**Uber:** Real-time trip data feeds into dynamic pricing
**Spotify:** Ingests user listening to power Wrapped + discovery
**Surfline** A surf app might ingest marine weather + wave height forecasts, store that in cloud storage, clean it, and send it to a dashboard that gives you alerts.

# 4Ô∏è‚É£ Core Components of a Modern Data Stack

| Area               | Tool                                |
|--------------------|-------------------------------------|
| **Ingestion**       | Python, Airflow, Kafka              |
| **Storage (Lake)**  | GCS, S3, Azure Blob                 |
| **Processing**      | PySpark, dbt, SQL                   |
| **Warehouse**       | BigQuery, Redshift, Snowflake       |
| **Orchestration**   | Airflow, Azure Data Factory         |
| **Serving layer**   | Power BI, Looker, Streamlit         |
| **DevOps / Infra**  | Docker, Terraform, CI/CD            |

## 5Ô∏è‚É£ Your Task: Sketch Your Pipeline

### ‚úçÔ∏è Exercise (10‚Äì15 min):
Draw your own version of a data pipeline for Your surf forecast app.

Include:
- Where the raw data comes from (API, files, db)
- Where it's stored raw (data lake)
- What kind of transformation might happen
- Where it's served to users

![SurfPipelineSketch](/images/SurfPipeline1.png)

## üß± The Core Aspects of Data Engineering

### 1. Data Ingestion - Getting data in from different sources.
- Could be from APIs, databases, files, message queues, or real-time streams.
- Example: Pulling surf data from an external API every hour.
- Tools: Python, Airbyte, Kafka, REST APIs, FTP, etc.

### 2. Data Storage - Saving the raw or processed data somewhere.
- Can be databases (PostgreSQL, BigQuery), data lakes (S3, GCS), or warehouses (Snowflake, Redshift).
- You choose depending on structure, size, cost, and access speed.
- Concepts:
  - Data Lakes (store everything)
  - Data Warehouses (structured & fast querying)

### 3. Data Transformation - Turning raw data into clean, usable formats.
- Cleaning, formatting, combining, deduplicating, aggregating, etc.
- This is where tools like SQL and dbt shine.
- ETL vs ELT:
  - ETL: Extract ‚Üí Transform ‚Üí Load
  - ELT: Extract ‚Üí Load ‚Üí Transform (modern approach)

### 4. Orchestration & Scheduling - Automating all the steps above on a regular basis.
- Chaining your jobs together and running them at the right times or in the right order.
- Example: ‚ÄúIngest ‚û° Transform ‚û° Load ‚û° Notify‚Äù
- Tools: Airflow, Prefect, Dagster

### 5. Monitoring & Reliability - Making sure pipelines don‚Äôt silently fail.
- Add logging, alerts, retry logic.
- Think like an engineer: if a pipeline fails at 2am, how do you know?
- Tools: Prometheus, Grafana, Airflow‚Äôs built-in monitoring

### 6. Infrastructure / DevOps (Optional but Powerful) - Knowing how your systems run, scale, and deploy.
- Not always required at junior level, but powerful to understand:
  - Docker for containerization
  - GitHub Actions / CI/CD for testing & deploying
  - Cloud (AWS, GCP) for hosting and scaling

## üß∞ Common Tech Stack
| Layer          | Tool Examples                                           |
|----------------|---------------------------------------------------------|
| Ingestion      | Python, Kafka, Airbyte                                  |
| Storage        | S3, PostgreSQL, BigQuery, Snowflake                     |
| Transformation | SQL, dbt, Pandas                                        |
| Orchestration  | Airflow, Prefect                                        |
| Monitoring     | Airflow UI, Prometheus, custom logs                     |
| Deployment     | Docker, Terraform, GitHub Actions                       |
| Cloud          | AWS (S3, Lambda, RDS), GCP (BigQuery, Cloud Functions)  |


## üë∂ Junior Data Engineer Role ‚Äì What You‚Äôll Typically Do
- Write basic data pipelines (Python scripts, SQL jobs)
- Maintain and monitor scheduled jobs
- Work with analysts or scientists to get them the data they need
- Debug data issues and fix broken pipelines
- Slowly grow into working on architecture, tooling, and infrastructure